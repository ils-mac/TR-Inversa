---
layout: default
title: Proyecto 7 ‚Äì Anotaci√≥n CONANP
parent: Semana 9
grand_parent: Unidad 3
nav_order: 4
---

# Proyecto 7 - Anotaci√≥n CONANP

**Puntos:** 3

**Modalidad:** Individual

## Descripci√≥n del Proyecto

En esta asignaci√≥n, realizar√°s una evaluaci√≥n profesional de calidad de traducci√≥n del documento _Logros - Proyecto Resiliencia_ de CONANP utilizando Label Studio. Esta ser√° tu primera evaluaci√≥n completa aplicando el marco MQM que has estado estudiando durante el semestre.

Como aprendiste en la p√°gina anterior, ya has revisado el contexto del proyecto, entiendes la misi√≥n de CONANP, y has documentado colaborativamente las especificaciones del proyecto en clase. Ahora aplicar√°s ese conocimiento para evaluar la calidad de la traducci√≥n proporcionada.

### Prop√≥sito de la Evaluaci√≥n

Los datos etiquetados son un recurso importante para entrenar sistemas de inteligencia artificial generativa a realizar tareas de traducci√≥n. A medida que el mundo se integra m√°s con IAG, un √°rea de oportunidad es aplicar tus habilidades de traducci√≥n a tareas de etiquetado de errores. La tipolog√≠a de errores MQM es un enfoque basado en est√°ndares para etiquetar errores que se est√° adoptando en toda la industria. Esta asignaci√≥n te dar√° experiencia tanto con MQM como con entornos profesionales de anotaci√≥n de errores.

Un aspecto importante del etiquetado de errores es que las y los evaluadores identifican errores de manera inconsistente al principio. Por lo tanto, todas y todos trabajar√°n con el mismo texto, y m√°s adelante estudiar√© c√≥mo se superponen‚Äîo no‚Äîsus anotaciones. Las observaciones sobre la superposici√≥n se utilizar√°n para ense√±arles sobre la armonizaci√≥n del trabajo de etiquetado de errores en equipo.

### Documentos del Proyecto

- **Texto fuente**:
   - Texto completo: [CONANP_LogrosProyectoResiliencia.pdf](https://github.com/alainamb/uic_tr18-trad-inversa-es-en/blob/main/unidad3/semana9/referencias/CONANP_LogrosProyectoResiliencia.pdf)
   - Adaptado para esta tarea: [ResilenciaM√©xico_ES.pdf](https://github.com/alainamb/uic_tr18-trad-inversa-es-en/blob/main/unidad3/semana9/referencias/ResilenciaM√©xico_ES.pdf)
- **Texto meta**:
   - Texto completo: [CONANP_ProyectoResilienciaAcheivements.pdf](https://github.com/alainamb/uic_tr18-trad-inversa-es-en/blob/main/unidad3/semana9/referencias/CONANP_ProyectoResilienciaAcheivements.pdf)
   - Adaptado para esta tarea: [ResilenciaM√©xico_EN.pdf](https://github.com/alainamb/uic_tr18-trad-inversa-es-en/blob/main/unidad3/semana9/referencias/ResilenciaM√©xico_EN.pdf)
- **Especificaciones del proyecto**: Documento compartido completado en clase (disponible en Teams)

## Tu Asignaci√≥n

### Paso 1: Revisar las Especificaciones

Antes de comenzar tu evaluaci√≥n, revisa cuidadosamente el documento de especificaciones que desarrollaron colaborativamente en clase. Estas especificaciones son la base para determinar:
- Qu√© constituye un error
- Qu√© nivel de severidad es apropiado
- Qu√© expectativas de calidad aplican para este proyecto espec√≠fico

### Paso 2: Acceder al Proyecto en Label Studio

1. Inicia sesi√≥n en Label Studio usando el enlace proporcionado en Teams
2. Localiza el proyecto: **"CONANP-Eval"**
3. Verifica que puedes ver el texto completo del documento

### Paso 3: Realizar la Evaluaci√≥n Completa

Aplica la metodolog√≠a que aprendiste durante las sesiones de entrenamiento tecnol√≥gico y ling√º√≠stico en clase. Tu evaluaci√≥n debe incluir:

#### A. Anotaciones de Errores Individuales

Para cada error que identifiques, debes proporcionar:

1. **Selecci√≥n precisa del texto**: Marca exactamente la parte problem√°tica del texto meta
2. **Dimensi√≥n del error**: Selecciona la dimensi√≥n apropiada (Terminolog√≠a, Precisi√≥n, Estilo, Convenciones Ling√º√≠sticas, Convenciones Locales, Adecuaci√≥n para la Audiencia, Otros)
3. **Tipo de error**: Especifica el tipo exacto dentro de la dimensi√≥n
4. **Nivel de severidad**: Asigna neutral, menor, mayor o cr√≠tica seg√∫n el impacto
5. **Comentarios**: Explica por qu√© es un error y, cuando sea posible, sugiere una correcci√≥n

**Ejemplo de anotaci√≥n completa:**
- **Texto marcado**: "tropicalization"
- **Dimensi√≥n**: Terminolog√≠a
- **Tipo**: T√©rmino incorrecto
- **Severidad**: Mayor
- **Comentario**: El t√©rmino establecido en ingl√©s para este fen√≥meno es "tropicalization" no "tropicalization". Seg√∫n las especificaciones, la terminolog√≠a debe ser consistente con la literatura cient√≠fica publicada.

#### B. Evaluaci√≥n Hol√≠stica del Documento

Despu√©s de identificar errores individuales, proporciona evaluaciones hol√≠sticas:

1. **Correspondencia (1-4)**: ¬øQu√© tan bien transmite el texto meta el significado del texto fuente?
   - 4 = Correspondencia excelente
   - 3 = Buena correspondencia
   - 2 = Correspondencia aceptable con problemas
   - 1 = Correspondencia pobre

2. **Legibilidad (1-4)**: ¬øQu√© tan bien funciona el texto meta como documento independiente en ingl√©s?
   - 4 = Muy legible y natural
   - 3 = Generalmente legible
   - 2 = Legible pero con problemas
   - 1 = Dif√≠cil de leer

3. **Comentarios hol√≠sticos**: Proporciona observaciones sobre:
   - Patrones de error que notaste
   - Fortalezas generales de la traducci√≥n
   - Problemas recurrentes
   - Efectividad general del documento

### Paso 4: Enviar Tu Evaluaci√≥n

Una vez que hayas completado tu evaluaci√≥n:
1. Revisa tu trabajo usando la lista de verificaci√≥n (ver m√°s abajo)
2. Haz clic en **"Submit"** en Label Studio para entregar tu evaluaci√≥n
3. No necesitas descargar ni subir ning√∫n archivo‚Äîtu trabajo se guarda autom√°ticamente en Label Studio

### Paso 5: Reflexi√≥n Escrita

Redacta una reflexi√≥n cr√≠tica (1-2 p√°rrafos) sobre tu experiencia trabajando con MQM en Label Studio. Tu reflexi√≥n debe abordar:

**Preguntas obligatorias:**
- ¬øQu√© funcion√≥ bien para ti en este proceso?
- ¬øQu√© desaf√≠os encontraste al categorizar errores?
- ¬øQu√© preguntas pendientes tienes sobre este trabajo?
- ¬øCrees que tu trabajo ha mejorado esta semana? ¬øPor qu√© o por qu√© no?

**Elementos de reflexi√≥n cr√≠tica** (incluye al menos 2):
- **An√°lisis de tu proceso**: ¬øC√≥mo abordaste la tarea? ¬øCambi√≥ tu estrategia mientras trabajabas?
- **Dificultades espec√≠ficas**: ¬øQu√© tipos de errores fueron m√°s dif√≠ciles de categorizar? ¬øPor qu√©?
- **Desarrollo de habilidades**: ¬øQu√© aprendiste sobre evaluaci√≥n de traducci√≥n que no sab√≠as antes?
- **Aplicaci√≥n futura**: ¬øC√≥mo influir√° esta experiencia en tu trabajo como traductor/a?
- **Limitaciones reconocidas**: ¬øD√≥nde reconoces que tu evaluaci√≥n podr√≠a ser subjetiva o inconsistente?
- **Comparaci√≥n con experiencias previas**: ¬øEn qu√© se diferencia evaluar el trabajo de otros vs. traducir t√∫ mismo/a?

**Entrega de la reflexi√≥n**: Sube tu reflexi√≥n como un documento en la tarea de Teams.

## Lista de Verificaci√≥n Pre-Entrega

Antes de hacer clic en "Submit" en Label Studio y entregar tu reflexi√≥n, verifica:

### Anotaciones en Label Studio
- [ ] He revisado el texto meta completo de manera sistem√°tica
- [ ] Cada error marcado tiene dimensi√≥n, tipo de error, severidad Y comentarios
- [ ] Mis comentarios explican claramente por qu√© algo es un error
- [ ] He considerado las especificaciones del proyecto al identificar errores
- [ ] No he marcado como errores las preferencias personales que no violan las especificaciones
- [ ] He proporcionado puntuaciones de correspondencia y legibilidad
- [ ] He escrito comentarios hol√≠sticos sobre el documento completo
- [ ] He hecho clic en **"Submit"** en Label Studio

### Reflexi√≥n Escrita
- [ ] Mi reflexi√≥n tiene 1-2 p√°rrafos (150-300 palabras aproximadamente)
- [ ] He abordado las cuatro preguntas obligatorias
- [ ] He incluido al menos dos elementos de reflexi√≥n cr√≠tica
- [ ] Mi reflexi√≥n va m√°s all√° de observaciones superficiales
- [ ] He revisado ortograf√≠a y gram√°tica
- [ ] He subido la reflexi√≥n como comentario en Teams

## Criterios de Evaluaci√≥n

### Puntuaci√≥n Total: 3 puntos

| **Componente** | **Criterio** | **Puntuaci√≥n** |
|----------------|--------------|----------------|
| **Completitud de anotaciones** (1 punto) | Excelente: Todas las anotaciones incluyen dimensi√≥n, tipo, severidad y comentarios detallados; evaluaci√≥n hol√≠stica completa<br><br>Bueno: La mayor√≠a de anotaciones est√°n completas; puede faltar informaci√≥n ocasionalmente<br><br>Insuficiente: Muchas anotaciones incompletas o falta evaluaci√≥n hol√≠stica | 1 punto<br><br>0.5 puntos<br><br>0 puntos |
| **Precisi√≥n de anotaciones** (1 punto) | Excelente: Identificaci√≥n precisa de errores; categorizaci√≥n apropiada seg√∫n MQM; niveles de severidad bien justificados<br><br>Bueno: Generalmente preciso con algunos errores de categorizaci√≥n; severidad mayormente apropiada<br><br>Insuficiente: Muchos errores mal categorizados; severidad inadecuada; confusi√≥n con preferencias personales | 1 punto<br><br>0.5 puntos<br><br>0 puntos |
| **Reflexi√≥n cr√≠tica** (1 punto) | Excelente: Reflexi√≥n profunda que demuestra pensamiento cr√≠tico; aborda todas las preguntas; incluye an√°lisis significativo del proceso<br><br>Bueno: Reflexi√≥n superficial pero completa; aborda las preguntas b√°sicamente; an√°lisis limitado<br><br>Insuficiente: Reflexi√≥n muy breve o superficial; no aborda las preguntas; falta pensamiento cr√≠tico | 1 punto<br><br>0.5 puntos<br><br>0 puntos |

### Notas Sobre la Evaluaci√≥n

**Completitud**: Se eval√∫a que todas las partes necesarias est√©n presentes. Una anotaci√≥n sin comentarios, por ejemplo, no cuenta como completa.

**Precisi√≥n**: Se eval√∫a qu√© tan bien aplicaste el marco MQM y las especificaciones del proyecto. No hay "respuestas correctas" absolutas, pero debe haber coherencia l√≥gica entre lo que marcaste como error, c√≥mo lo categorizaste, y las especificaciones del proyecto.

**Reflexi√≥n cr√≠tica**: Se eval√∫a la profundidad de tu pensamiento, no solo el cumplimiento con el formato. Una reflexi√≥n excelente demuestra que est√°s procesando activamente lo que aprendiste y c√≥mo se relaciona con tu desarrollo profesional.

## Recursos de Apoyo

### Durante Tu Evaluaci√≥n

- **Tipolog√≠a MQM**: [themqm.org/the-mqm-typology](https://themqm.org/the-mqm-typology/)
- **Marco MQM del curso**: [Semana 4 - Marco MQM](../../unidad2/semana4/marco-mqm.md)
- **Ejemplos de errores MQM:** Documento compartido en Teams
- **Especificaciones del proyecto**: Documento compartido en Teams
- **Archivos de apoyo**: [Week 9 CONANP Evaluation Files en Teams](https://universidaduic.sharepoint.com/:f:/s/TR18TraduccinInversaAB-2025/EjRB12Q5ajpBoKAnOsCysPsBt3C0KSbFq2CIRdk5nG9RhA?e=5Sovch)

### Si Tienes Dudas

- **Durante la clase**: Pregunta a la profesora mientras trabajas
- **Fuera de la clase**: Env√≠a un mensaje en Teams o correo electr√≥nico
- **Dudas t√©cnicas con Label Studio**: Consulta la [Gu√≠a de etiquetado](https://labelstud.io/guide/labeling)
- **Dudas sobre MQM**: Revisa la [p√°gina de MQM de la semana 4](../../unidad2/semana4/marco-mqm.md)

No dudes en pedir aclaraciones‚Äîes mejor preguntar que hacer suposiciones incorrectas.

## Entrega

### Componentes Requeridos

1. **Evaluaci√≥n en Label Studio**
   - Completada en el proyecto "CONANP-Eval with better UX"
   - Debe hacer clic en "Submit" para entregar
   - No hay archivo separado que descargar/subir

2. **Reflexi√≥n escrita**
   - 1-2 p√°rrafos (150-300 palabras)
   - Subida como comentario en la tarea de Teams
   - Fecha l√≠mite: [Fecha indicada en Teams]

### Formato de la Reflexi√≥n

Tu reflexi√≥n debe ser presentada profesionalmente:
- P√°rrafos bien organizados
- Ortograf√≠a y gram√°tica correctas
- Pensamiento claro y coherente
- Evidencia de reflexi√≥n cr√≠tica profunda

## ¬øQu√© Sigue? An√°lisis de Concordancia entre Evaluadores/as

Despu√©s de que todas y todos hayan completado sus evaluaciones, se realizar√° un an√°lisis de **concordancia entre evaluadores/as** (inter-annotator agreement). Este an√°lisis medir√°:

- **Concordancia exacta**: ¬øQu√© tan frecuentemente marcaron exactamente el mismo texto como error?
- **F1 para coincidencia parcial**: ¬øQu√© tan bien se superponen sus anotaciones cuando no son id√©nticas?
- **Kappa para concordancia de categor√≠as**: ¬øQu√© tan consistentemente categorizaron el mismo tipo de error?

### ¬øPor Qu√© Importa la Concordancia?

La concordancia entre evaluadores/as es un indicador de:
- **Calidad de las anotaciones**: Alta concordancia sugiere evaluaciones objetivas y bien fundamentadas
- **Comprensi√≥n compartida**: Medida de qu√© tan bien entienden y aplican MQM como grupo
- **√Åreas de mejora**: Baja concordancia en ciertos tipos de errores indica necesidad de capacitaci√≥n adicional

Los resultados se discutir√°n en clase y se utilizar√°n para:
- Identificar √°reas donde el grupo necesita armonizar su comprensi√≥n
- Desarrollar mejores pr√°cticas para evaluaci√≥n consistente
- Mejorar la aplicaci√≥n del marco MQM en futuras evaluaciones

Este proceso de medici√≥n y armonizaci√≥n refleja pr√°cticas profesionales reales en la industria de localizaci√≥n, donde equipos de evaluadores/as deben trabajar de manera consistente para proporcionar retroalimentaci√≥n √∫til y datos confiables.

## Reflexi√≥n Final

Esta evaluaci√≥n representa la culminaci√≥n de tu aprendizaje sobre gesti√≥n de calidad de traducci√≥n en este curso. Has progresado desde:
- Ser evaluado/a como traductor/a
- Comprender el marco MQM te√≥ricamente
- Practicar con el entorno de Label Studio
- Realizar una evaluaci√≥n profesional completa

Las habilidades que desarrollaste‚Äîidentificar errores sistem√°ticamente, categorizarlos con precisi√≥n, y evaluar hol√≠sticamente‚Äîson fundamentales para el trabajo profesional en traducci√≥n, localizaci√≥n y entrenamiento de sistemas de IA.

---

## üì• Descarga este Contenido

Encuentra este archivo [en nuestro repositorio](https://github.com/alainamb/uic_tr18-trad-inversa-es-en/blob/main/unidad3/semana9/proyecto7-Anotaci√≥nCONANP.md) y desc√°rgalo.

### ü§ñ Prompts de Estudio con IAG
Copia el contenido descargado y prueba estos prompts:
- "Ay√∫dame a desarrollar una estrategia sistem√°tica para revisar un documento completo e identificar todos los errores de traducci√≥n"
- "¬øC√≥mo puedo distinguir entre errores reales que violan las especificaciones y preferencias personales en traducci√≥n?"
- "Crea un marco para escribir comentarios √∫tiles y constructivos sobre errores de traducci√≥n"
- "¬øQu√© preguntas debo hacerme al determinar el nivel de severidad apropiado para un error de traducci√≥n?"
- "Ay√∫dame a reflexionar cr√≠ticamente sobre mi proceso de evaluaci√≥n: ¬øqu√© funcion√≥ y qu√© podr√≠a mejorar?"
- "Explica c√≥mo las habilidades de evaluaci√≥n de calidad pueden mejorar mi trabajo como traductor/a"

---

**¬°Unidad 3 Completa!** Repasa lo que has logrado en la [Conclusi√≥n de la Unidad 3](../unidad3-conclusion.md)
